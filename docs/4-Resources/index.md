# Learning Resources


Starting new learning journeys can feel overwhelming. I find that using [markmaps](https://markmap.js.org/) helps me organize and pace my learning in a way that makes sense to me. Want to learn Fine Tuning from concepts to code to cloud? Let's Go!

Here's an interactive markmap.

```markmap

# [Go! üèÅ]())

## 1. Overview

### 1.1 What is Fine Tuning?
### 1.2 Why use Fine Tuning?
### 1.3 When is Fine Tuning useful?
### 1.4 How do I get started?

## 2. Fine Tuning Concepts

### 2.1 [Fine Tuning Variants](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning?tabs=azure-openai%2Cturbo%2Cpython-new&pivots=rest-api)
### 2.2 [Fine Tuning & Function Calling](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning-functions)
### 2.3 [Evaluation & Experimentation](https://techcommunity.microsoft.com/blog/aiplatformblog/announcing-model-fine-tuning-collaborations-weights--biases-scale-ai-gretel-and-/4289514) - Example: [Statsig](https://docs.statsig.com/azureai/introduction/) 
### 2.4 [Synthetic Data Generation - Azure](https://github.com/Azure/synthetic-qa-generation) - [by Gretel](https://docs.gretel.ai/) - [Azure Fine-Tuning](https://gretel.ai/blog/privacy-preserving-ai-development-with-azure-gretel) - [Azure RAG Evaluation](https://gretel.ai/blog/gretel-ai-branch-blog-rag-model-evaluation-with-azure-ai-and-gretel-navigator)

## 3. Fine Tuning Process

### 3.1 Data Gathering & Formatting
### 3.2 Model Training & Complexity
### 3.3 Model Evaluation & Iteration
### 3.4 Model Deployment & Monitoring

## 4. Fine Tuning in Azure AI Foundry

### 4.1 [Fine Tuning Overview](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/fine-tuning-overview)
### 4.2 [Using Managed Compute](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/fine-tune-managed-compute)
### 4.3 [For OpenAI Models](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning) - via Portal, SDK, REST
### 4.4 [Distillation (transfer learning)](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/concept-model-distillation) - via Portal

## 5. Fine Tuning in Open AI

### 5.1 [Fine Tuning](https://platform.openai.com/docs/guides/fine-tuning)

### 5.2 [Model Distillation](https://platform.openai.com/docs/guides/distillation)

### 5.3 [Fine Tuning gpt-4o](https://openai.com/index/gpt-4o-fine-tuning/)

### 5.4 Fine Tuning Cookbook
- [Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb)
- [Distillation](https://cookbook.openai.com/examples/leveraging_model_distillation_to_fine-tune_a_model)
- [Vision Fine-Tuning gpt-4o](https://cookbook.openai.com/examples/multimodal/vision_fine_tuning_on_gpt4o_for_visual_question_answering)
- [Fine Tuning for Function Calling](https://cookbook.openai.com/examples/fine_tuning_for_function_calling)
- [Fine Tuning for RAG with QDRANT](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant)
- 


## 6. Fine Tuning in Hugging Face

### 6.1 [Fine-Tune a Pretrained Model](https://huggingface.co/docs/transformers/training)
### 6.2 [Fine-Tuning TinyLLama for Text Generation](https://huggingface.co/blog/nroggendorff/finetune-tinyllama)
### 6.3 [Efficient Training on CPU](https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_cpu)
### 6.4 [Fine Tuning QA Example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)

## 7. Dev Tools & Best Practices

### 7.1 [Weights & Biases Integration](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/weights-and-biases-integration)
### 7.2 [Fine-Tune Meta Llama Models](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/fine-tune-model-llama?tabs=llama-three%2Cchatcompletion) - via Portal
### 7.3 [Fine-Tune Phi-3 Models](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/fine-tune-phi-3?tabs=phi-3-mini) - via Portal
### 7.4 [Fine-Tune with Unsloth](https://github.com/unslothai/unsloth) - for various models
### 7.5 [Synthetic Data Gen with Tuna](https://blog.langchain.dev/introducing-tuna-a-tool-for-rapidly-generating-synthetic-fine-tuning-datasets/)

## 8. Resources & Wrap-up

1. [Ultimate Guide to Fine-Tuning LLMs](https://arxiv.org/html/2408.13296v1#Ch6) - Aug 2024, arXiv.org
1. [A Comprehensive Guide to Fine-Tuning Large Language Models](https://www.analyticsvidhya.com/blog/2023/08/fine-tuning-large-language-models/) - Nov 2024, Analytics Vidhya
1. [Fine-tuning large language models (LLMs) in 2024](https://www.superannotate.com/blog/llm-fine-tuning) - Jul 2024, SuperAnnotate


```

